wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: orionfoo (orionfoo-massachusetts-institute-of-technology). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.0
wandb: Run data is saved locally in /workspace/effective_transfer_learning/wandb/run-20241207_193628-sc9pohpa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-bee-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/orionfoo-massachusetts-institute-of-technology/deep_learning
wandb: üöÄ View run at https://wandb.ai/orionfoo-massachusetts-institute-of-technology/deep_learning/runs/sc9pohpa
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([102]) in the model instantiated
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([102, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
experiment:
  name: deep_learning
  seed: 42
  device: cuda
model: null
dataset:
  name: Flowers102
  path: ob
  batch_size: 128
  val_batch_size: 512
  train_split: 0.8
training:
  epochs: 80
  learning_rate: 0.0001
  patience: 5
  early_stopping: true
optimizer:
  type: Adam
  weight_decay: 1.0e-06
lr_scheduler:
  factor: 0.1
  min_lr: 1.0e-05
logging:
  wandb:
    project: deep_learning
    enabled: true

ViTForImageClassification(
  (vit): ViTModel(
    (embeddings): ViTEmbeddings(
      (patch_embeddings): ViTPatchEmbeddings(
        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): ViTEncoder(
      (layer): ModuleList(
        (0-11): 12 x ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (classifier): Linear(in_features=768, out_features=102, bias=True)
)
  0%|          | 0/13 [00:00<?, ?it/s]  8%|‚ñä         | 1/13 [00:05<01:03,  5.26s/it] 15%|‚ñà‚ñå        | 2/13 [00:10<00:59,  5.44s/it] 23%|‚ñà‚ñà‚ñé       | 3/13 [00:16<00:54,  5.45s/it] 31%|‚ñà‚ñà‚ñà       | 4/13 [00:23<00:54,  6.05s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [00:29<00:48,  6.08s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:35<00:43,  6.25s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [00:41<00:37,  6.17s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:49<00:33,  6.64s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9/13 [00:55<00:25,  6.43s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10/13 [01:02<00:19,  6.51s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11/13 [01:07<00:12,  6.12s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12/13 [01:13<00:05,  5.94s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [01:13<00:00,  5.63s/it]
  0%|          | 0/8 [00:00<?, ?it/s] 12%|‚ñà‚ñé        | 1/8 [00:03<00:22,  3.18s/it] 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.43s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:07<00:11,  2.20s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:09<00:09,  2.27s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:11<00:06,  2.17s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:14<00:04,  2.42s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:16<00:02,  2.21s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:17<00:00,  2.04s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:17<00:00,  2.22s/it]
torch.Size([128, 102])
torch.Size([128, 102])
torch.Size([128, 102])
torch.Size([128, 102])
torch.Size([128, 102])
torch.Size([128, 102])
torch.Size([128, 102])
torch.Size([124, 102])
  0%|          | 0/13 [00:00<?, ?it/s]  8%|‚ñä         | 1/13 [00:05<01:06,  5.52s/it]