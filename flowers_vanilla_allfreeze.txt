wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: orionfoo (orionfoo-massachusetts-institute-of-technology). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.0
wandb: Run data is saved locally in /workspace/effective_transfer_learning/wandb/run-20241207_193714-2cfallro
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-music-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/orionfoo-massachusetts-institute-of-technology/deep_learning
wandb: üöÄ View run at https://wandb.ai/orionfoo-massachusetts-institute-of-technology/deep_learning/runs/2cfallro
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([102]) in the model instantiated
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([102, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
experiment:
  name: deep_learning
  seed: 42
  device: cuda
model: null
dataset:
  name: Flowers102
  path: ob
  batch_size: 128
  val_batch_size: 512
  train_split: 0.8
training:
  epochs: 80
  learning_rate: 0.0001
  patience: 5
  early_stopping: true
optimizer:
  type: Adam
  weight_decay: 1.0e-06
lr_scheduler:
  factor: 0.1
  min_lr: 1.0e-05
logging:
  wandb:
    project: deep_learning
    enabled: true

ViTForImageClassification(
  (vit): ViTModel(
    (embeddings): ViTEmbeddings(
      (patch_embeddings): ViTPatchEmbeddings(
        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): ViTEncoder(
      (layer): ModuleList(
        (0-11): 12 x ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (classifier): Linear(in_features=768, out_features=102, bias=True)
)
  0%|          | 0/13 [00:00<?, ?it/s]  8%|‚ñä         | 1/13 [00:05<01:11,  5.99s/it] 15%|‚ñà‚ñå        | 2/13 [00:12<01:08,  6.23s/it] 23%|‚ñà‚ñà‚ñé       | 3/13 [00:19<01:04,  6.49s/it] 31%|‚ñà‚ñà‚ñà       | 4/13 [00:24<00:55,  6.17s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [00:31<00:50,  6.31s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:37<00:42,  6.07s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [00:42<00:35,  5.97s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8/13 [00:48<00:29,  5.97s/it]